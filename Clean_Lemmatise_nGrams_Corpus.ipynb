{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean & Lemmatise Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data \n",
    "with open('phys_all.txt', 'r') as infile:\n",
    "    data_dic = json.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# get the abstracts for certain years\n",
    "abstracts_1995 = []\n",
    "abstracts_2005 = []\n",
    "abstracts_2015 = []\n",
    "for key in data_dic.keys(): \n",
    "    if data_dic[key][\"year\"] in [1994, 1995, 1996]:\n",
    "        abstracts_1995.append(data_dic[key][\"abstract\"])\n",
    "    elif data_dic[key][\"year\"] in [2004, 2005, 2006]:\n",
    "        abstracts_2005.append(data_dic[key][\"abstract\"])\n",
    "    elif data_dic[key][\"year\"] in [2014, 2015, 2016]:\n",
    "        abstracts_2015.append(data_dic[key][\"abstract\"])\n",
    "    else: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_n_grams_lemma(abstracts_list): \n",
    "\n",
    "    # Split doc to tokens.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(abstracts_list)):\n",
    "        abstracts_list[idx] = abstracts_list[idx].lower()  # Convert to lowercase.\n",
    "        abstracts_list[idx] = tokenizer.tokenize(abstracts_list[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers\n",
    "    docs_filter1 = [[token for token in doc if not token.isnumeric()] for doc in abstracts_list]\n",
    "\n",
    "    # Remove words less than 2 characters\n",
    "    docs_filter2 = [[token for token in doc if len(token) > 1] for doc in docs_filter1]\n",
    "\n",
    "    # Lemmatise\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs_lemm = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs_filter2]\n",
    "\n",
    "    # Compute bigrams and trigrams\n",
    "    from gensim.models import Phrases\n",
    "    bigram = gensim.models.Phrases(docs_lemm, min_count=10) #, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[docs_lemm], min_count=10) #, threshold=100)  \n",
    "\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "    for id_x in range(len(docs_lemm)):\n",
    "        for token in trigram_mod[bigram_mod[docs_lemm[id_x]]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram or trigram and will be added to the documents\n",
    "                docs_lemm[idx].append(token)\n",
    "\n",
    "    # Remove stop words\n",
    "    docs_lemm_clean = [[token for token in doc if token not in stop_words] for doc in docs_lemm]    \n",
    "    \n",
    "    return docs_lemm_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_lemm_clean_phys_1995 = tokenise_n_grams_lemma(abstracts_1995)\n",
    "docs_lemm_clean_phys_2005 = tokenise_n_grams_lemma(abstracts_2005)\n",
    "docs_lemm_clean_phys_2015 = tokenise_n_grams_lemma(abstracts_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['describe',\n",
       " 'simple',\n",
       " 'model',\n",
       " 'evolution',\n",
       " 'incorporates',\n",
       " 'branching',\n",
       " 'extinction',\n",
       " 'specie',\n",
       " 'line',\n",
       " 'also',\n",
       " 'includes',\n",
       " 'abiotic',\n",
       " 'influence',\n",
       " 'first',\n",
       " 'principle',\n",
       " 'approach',\n",
       " 'taken',\n",
       " 'probability',\n",
       " 'speciation',\n",
       " 'extinction',\n",
       " 'defined',\n",
       " 'purely',\n",
       " 'term',\n",
       " 'fitness',\n",
       " 'landscape',\n",
       " 'specie',\n",
       " 'numerical',\n",
       " 'simulation',\n",
       " 'show',\n",
       " 'total',\n",
       " 'diversity',\n",
       " 'fluctuates',\n",
       " 'around',\n",
       " 'natural',\n",
       " 'system',\n",
       " 'size',\n",
       " 'n_',\n",
       " 'rm',\n",
       " 'nat',\n",
       " 'weakly',\n",
       " 'depends',\n",
       " 'upon',\n",
       " 'number',\n",
       " 'connection',\n",
       " 'per',\n",
       " 'specie',\n",
       " 'agreement',\n",
       " 'known',\n",
       " 'data',\n",
       " 'real',\n",
       " 'multispecies',\n",
       " 'community',\n",
       " 'numerical',\n",
       " 'result',\n",
       " 'confirmed',\n",
       " 'approximate',\n",
       " 'mean',\n",
       " 'field',\n",
       " 'analysis',\n",
       " 'which_incorporates',\n",
       " 'first_principle',\n",
       " 'numerical_simulation',\n",
       " 'show_that',\n",
       " 'n__rm',\n",
       " 'only_weakly',\n",
       " 'depends_upon',\n",
       " 'agreement_with',\n",
       " 'numerical_result',\n",
       " 'mean_field']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_lemm_clean_phys_1995[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30145, 111737, 176383)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_lemm_clean_phys_1995), len(docs_lemm_clean_phys_2005), len(docs_lemm_clean_phys_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many words does the corpus have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in corpus for different years:\n",
      "1995 : 2265384\n",
      "2005 : 9783946\n",
      "2015 : 19108495\n"
     ]
    }
   ],
   "source": [
    "datas = [docs_lemm_clean_phys_1995, docs_lemm_clean_phys_2005, docs_lemm_clean_phys_2015]\n",
    "years = [1995, 2005, 2015]\n",
    "\n",
    "print(\"Words in corpus for different years:\")\n",
    "for year, data in zip(years, datas):\n",
    "    counter = 0\n",
    "    for i in range(len(data)): \n",
    "        counter += len(data[i])\n",
    "    print(year,\":\", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data to file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs_lemm_clean_phys_1995.txt', 'w') as outfile:\n",
    "    json.dump(docs_lemm_clean_phys_1995, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs_lemm_clean_phys_2005.txt', 'w') as outfile:\n",
    "    json.dump(docs_lemm_clean_phys_2005, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs_lemm_clean_phys_2015.txt', 'w') as outfile:\n",
    "    json.dump(docs_lemm_clean_phys_2015, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which documents were published when "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "    \n",
    "# abstracts for years\n",
    "abs2year_dic = {}\n",
    "for num, key in enumerate(data_dic.keys()): \n",
    "    tem_dic = {}\n",
    "    abstract = tokenise_n_grams_lemma([data_dic[key][\"abstract\"]])\n",
    "    tem_dic[\"abstract\"] = abstract[0]\n",
    "    tem_dic[\"year\"] = data_dic[key][\"year\"]\n",
    "    abs2year_dic[num] = tem_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data to a file\n",
    "with open('abs2year_dic.txt', 'w') as outfile:\n",
    "    json.dump(abs2year_dic, outfile)\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
